# Project Title

Implementing a translation model using Sequence-to-Sequence architecture.

## Description
According to following research paper in 'Sequence to Sequence Learning with Neural Networks',

"It's a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Here a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector."

## Getting Started

### Dependencies

* Python 3.x
* PyTorch
* Linux

### Executing program

To train the model I followed the steps explained in the following article. 

## License

This project is licensed under the MIT License.

## Acknowledgments

Inspiration, code snippets, etc.
* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
* [Sequnce to Sequence with pyTorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
