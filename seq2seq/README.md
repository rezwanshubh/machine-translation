# Project Title

Implementing a translation model using Sequence-to-Sequence architecture.

## Description

Last few years we have observed tremendous performance by Sequence-to-Sequence (seq2seq) modeling for machine translation. In the curve of this improvement, we have also been overwhelmed by another translation model named Transformer. In this following research paper, researchers combined all the latest training techniques into RNMT+ architecture. In this hybrid model, they showed that this approach outperforms all previous models for translation pairs of English-French or English-German. 

However, in our implementation, we will use Relational-RNN instead of LSTM and evaluate the performance of the English-Estonian language pair.


## Getting Started

### Dependencies

* Python 3.x
* PyTorch
* Linux

### Executing program

* How to run the program
* Step-by-step bullets
```
code blocks for commands
```

## License

This project is licensed under the MIT License.

## Acknowledgments

Inspiration, code snippets, etc.
* [Combining Recent Advances in Neural Machine Translation](https://www.aclweb.org/anthology/P18-1008/)
* [Relational recurrent neural networks](https://arxiv.org/abs/1806.01822)
* [nmtlab](https://github.com/zomux/nmtlab)
* [relational-rnn-pytorch](https://github.com/L0SG/relational-rnn-pytorch)
