# Project Title

Implementing a translation model using Transformer architecture.

## Description

In Transformer architecture, researchers propose a new simple network architecture, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

To develop this model, we used OpenNMT-py a pyTorch port for OpenNMT. To tokenize the text, we used SentencePiece, an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems.

## Getting Started

### Dependencies

* Python 3.x
* OpenNMT-py
* SentencePiece
* Linux

### Executing program

To train a translation model using Transformer architecture, we followed the procedures explained in the following project.

[Building a translation model using OpenNMT-py](https://opennmt.net/OpenNMT-py/quickstart.html)

## License

This project is licensed under the MIT License.

## Acknowledgments

Inspiration, code snippets, etc.
* [Attention is All You Need](https://arxiv.org/abs/1706.03762)
* [OpenNMT-py](https://opennmt.net/OpenNMT-py/)
* [SentencePiece](https://github.com/google/sentencepiece)
